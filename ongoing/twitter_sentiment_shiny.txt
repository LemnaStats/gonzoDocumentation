I want to create a shiny app for my past project on twitter sentiment. There are some caveats:

* I have never made a shiny app before
* I don't know how to do this

The reason I want to do it with Shiny is because the original functions were written in R. I feel like that's the most natural way to do it, or it 
has to be. Not sure though.

How am I going to go about this?

1. Plan the site visually
2. Review the scripts I currently
3. Make the site according to the plan

I am also going to use Github this time around! It'll make it easier for me to review my code.

First, let's upload what I currently have to a github repository.

https://github.com/LemnaStats/TwitterSentiment

Done!

Now let's design the site. I'm looking to take the following information:

* A a single search query
* A list of filtered words

And spit out:

* Sentiment score
* A word cloud
* A bar graph (pie chart?) of the number of positive, negative, and neutral tweets of the 100
* A download link

PART 1: Make plan

The first step I want to do is create a visual plan for the app. I'm going to do this in inkscape.

https://github.com/LemnaStats/TwitterSentiment/blob/main/twitterSentimentMap.png

Done! Now I have  I've also added two additional output fields: an average and median sentiment score.

PART 2: Build the underlying functionality

I would like to build out the underlying functions first before I put together the website.

The first step will be to create an input for the list of filtered words. I'm going to break this down into two steps:

1. Take a string with semicolon separation and make it into a string vector
2. Redo the scripts I currently have to take the string vector as an input

I'm going to modify the text amalgamator to take a filter:

text_amalgamator <- function(data,filters){
scorp_words <- data$text
scorp <- str_replace_all(scorp_words, "[^[:alnum:]]", " ")
scorp <- paste(scorp, collapse = " ")
scorp <- str_split(scorp," ") %>% unlist() %>% as.data.frame()
colnames(scorp) <- c("word")
scorp$word <- tolower(scorp$word)
bad <- filters
scorp <- scorp %>% filter(., !(word %in% bad))
scorp_count <- scorp %>% count(word)
scorp_count

scorp_count <- merge.data.frame(scorp_count,afinn)
scorp_count <- mutate(scorp_count, score = n * value)
scorp_count <- tibble(scorp_count)
return(scorp_count) }

And I'm going to use - wait for it - github features to keep track of this! I'm going to make this as a pull request after testing that it works.

Let's review the various machines I have working right now:

* searchTwitterTextAndTimeStamp, which pulls tweets from twitter using my API keys. Takes a search and a pull count, up to 100. Outputs a table of tweets.
* text_amalgamator, which takes the twitter output and makes it into a sentiment table. Takes a table of tweets and a filtered list. Outputs a sentiment table
* twitter_sentiment, which combines the two functions above. Takes a search and a filtered word list. Outputs a sentiment table.
* count_tester, which determines what words should be counted for the yes/no counts
* conduct_sentiment, which takes a query and a filtered word list and outputs a line analysing the sentiment for the search
* analyse_sentiment, which takes a query and a sentiment table and outputs a line analysing the sentiment for the search
* many_sentiments, which takes a tibble of queries and a filtered word list and outputs a table of sentiment lines (via analyse_sentiment), a sentiment table for each search, and a table of tweets for each search

Sentiment lines, for my own reference, contain:

* The query
* The count of meaningful words
* A count of yeses
* A count of nos
* An overall sentiment score score
* A list of common words

What I want is one function that takes:

* A search query (one)
* A filtered word list

And outputs:

* A sentiment score - DONE
* A count of emotionally-charged words - DONE
* A count of tweets (these two points will be linked together in a sentence - "found n sentiment-indicating words in m tweets")
* An average sentiment score
* A median sentiment score
* A bar chart of the % of tweets that are positive, negative, and neutral
* A word cloud of the most common words
* The table of tweets - DONE
* The sentiment table - DONE

I'm thinking of doing a histogram instead of the bar chart and a bar chart instead of the word cloud. We'll run some tests and see.

I've confirmed that the many_sentiments function still works. I'll use this function as the base. It can take a single query or multiple, so I'll just leave it as unclean for now. 

The tweet count is easy - I just throw this line into the analyse_sentiments function:

  gem_tibble$tweet_count <- nrow(tweets)
  
The average sentiment score also easy:

  gem_tibble$tweet_count <- nrow(tweets)

Our checklist is now:

* A sentiment score - DONE
* A count of emotionally-charged words - DONE
* A count of tweets (these two points will be linked together in a sentence - "found n sentiment-indicating words in m tweets") - DONE
* An average sentiment score - DONE
* A median sentiment score
* A bar chart of the % of tweets that are positive, negative, and neutral
* A word cloud of the most common words
* The table of tweets - DONE
* The sentiment table - DONE

That's fun!

Now I'm going to have to go higher up and calculate the sentiment for each invidual tweet. The goal here is:
* Take an individual string (a tweet) and split it up into words
* Match each word to the AFINN list
* Sum the total score

Text_amalgamator already does most of this, so let's modify it to take a string and spit out a single variable.

single_tweet_sentiment <- function(string,bad){
  scorp_words <- string
  afinn <- get_sentiments("afinn")
  scorp <- str_replace_all(scorp_words, "[^[:alnum:]]", " ")
  scorp <- paste(scorp, collapse = " ")
  scorp <- str_split(scorp," ") %>% unlist() %>% as.data.frame()
  colnames(scorp) <- c("word")
  scorp$word <- tolower(scorp$word)
  scorp <- scorp %>% filter(., !(word %in% bad))
  scorp_count <- scorp %>% count(word)
  scorp_count <- merge.data.frame(scorp_count,afinn)
  scorp_count <- mutate(scorp_count, score = n * value)
  return(scorp_count$score) }
  
  Hacky but it'll work.
  
  I wrote the above text yesterday, and then spent the rest of the day figuring out that it, in fact, did not work. I did not run the right tests on it. It spit out a bunch of individual scores for words instead of one overall score.
  
  I fixed it:
  
  single_tweet_sentiment <- function(string,bad){
  scorp_words <- string
  afinn <- get_sentiments("afinn")
  scorp <- str_replace_all(scorp_words, "[^[:alnum:]]", " ")
  scorp <- paste(scorp, collapse = " ")
  scorp <- str_split(scorp," ") %>% unlist() %>% as.data.frame()
  colnames(scorp) <- c("word")
  scorp$word <- tolower(scorp$word)
  scorp <- scorp %>% filter(., !(word %in% bad))
  scorp_count <- scorp %>% count(word)
  scorp_count <- merge.data.frame(scorp_count,afinn)
  scorp_count <- mutate(scorp_count, score = n * value)
  final_score <- scorp_count$score %>% sum()
  return(final_score) }
  
  And then I vectorized it:

stsvect <- Vectorize(single_tweet_sentiment, vectorize.args = "string")

Vectorization being a thing I only learned to do upon researching why my mutate() wasn't working.

Let's modify many_sentiments to add this to the tweets table:

sentiment_cannon <- function(query_list,filtered_words){
  #setup
    results_tibble <- tibble(query=character(),word_count=integer(),
                             yes_count=integer(),no_count=integer(),
                             score=integer(),common_words=character())
    tweet_table_list <- list(results_tibble)
    names(tweet_table_list) <- c(query_list[[1]])
    sentiment_table_list <- list(results_tibble)
    names(sentiment_table_list) <- c(query_list[[1]])
  
  #core cannon
    tweets <- searchTwitterTextAndTimestamp(item,100)
    tweets <- mutate(tweets, score = stsvect(text,filtered_words))
    sentis <- text_amalgamator(tweets,filtered_words)
    new_row <- analyse_sentiment(item,sentis,tweets,filtered_words)
    results_tibble <- rbind(results_tibble,new_row)
    tweet_table_list[[item]] <- tweets
    sentiment_table_list[[item]] <- sentis
    
  #returning results  
  master_list <- list(results_tibble,tweet_table_list,sentiment_table_list)
  names(master_list) <- c("sentiments","tweets","word lists")
  return(master_list)
}

My list is now:
* A sentiment score - DONE
* A count of emotionally-charged words - DONE
* A count of tweets (these two points will be linked together in a sentence - "found n sentiment-indicating words in m tweets") - DONE
* An average sentiment score - DONE
* A median sentiment score - DONE
* A bar chart of the % of tweets that are positive, negative, and neutral
* A word cloud of the most common words
* The table of tweets - DONE
* The sentiment table - DONE

I think I'm going to switch up the charts. I'm going to to a histogram for the positive/negative tweets. For the words I think a sideways bar chart should work.

The histogram is easy, just a matter of throwing this in the function:

  #sentiment histogram
    sent_histogram <- ggplot(tweets, aes(x=score)) + 
      geom_histogram(binwidth = 1, color= "black",fill='white') +
      ggtitle(str_glue("Sentiment Scores for tweets containing: ",query)) +
      xlab("Sentiment Score") +
      ylab("Number of Tweets")
      
The sideways bar chart is going to be a little harder. I want to color the bar based on the strength of the sentiment. 

After some finagling, this is the final function:

    topten <- sentis %>% arrange(desc(n)) %>% head(n=10) %>% 
      ggplot(aes(reorder(word,n),n,fill=value)) + 
      geom_col() + coord_flip() +
      ylab('Frequecy of word in tweet sample') +
      xlab("Top 10 Words")
      
That works!

We now have everything we need. It's time to make a shiny app.

PART 3: Make the shiny app

I have never made a shiny app before. First time for everything!

It seems like the process is pretty simple. It takes a bunch of inputs and plops them into an input table. It takes a bunch of outputs and plops them into an output table. So I have to take the two inputs, a bunch of outputs, and make it all work.

Let's start with the two inputs.

        textInput("query",NULL,placeholder="Enter search terms here"),


        textInput("filters",NULL,placeholder =
                      "Filtered Words (separate with a semicolon,e.g.'bread,toast'"),
                      

That works, but the entry bars are narrow. Whatever. I'll make sure the functionality works before making it pretty.

Feb 5

The problem is that the functionality doesn't work. The app literally will not load unless I take out the operational features on the server side.

I keep getting this error:
  cannot coerce type 'closure' to vector of type 'character'
  
I got a different error before adding wrappers to the input variable. I am losing my mind.

Based on further research the issue is something to do with my call to the function. I have tried wrapping the function in reactiveUI(), but now I can't call the function. Also my button doesn't work. Frustrating!

Upon further research, it looks like I need to write the data somewhere. This means that now the button has to:

* Call the function and generate data
* Write data somewhere
* Read the data back in
* Generate outputs

Once again, I don't know how I'm going to do this.

Let's see what the internet says!

https://shiny.rstudio.com/articles/persistent-data-storage.html

Given that I export my output as a list, it would be best if I stored my output in a list-friendly filetype. Ideally I want to save all of the data in one file in one go.
